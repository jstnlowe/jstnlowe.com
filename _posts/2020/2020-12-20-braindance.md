---
layout: single
title: "Build Your Own Braindance"
date: 2020-12-20 22:05:40 -0400
categories: design
published: true
---

Cyberpunk 2077 has this in-game concept of a “braindance” where special implants record every sensory input (and some that are extra-sensory, such as thermal infrared) that a person is experiencing. The recording can then be played back by another person, allowing them to experience the events from a first-person perspective or even explore the recorded environment as if they were actually there. This is pretty much directly inspired by the SQUIDs in the movie [Strange Days](https://en.wikipedia.org/wiki/Strange_Days_(film)) and there are a lot of references to the movie throughout the game.

We actually have a primitive form of this available to us today and some aspects of it can be seen in CP2077’s interpretation of the technology. It’s a process called [photogrammetry](https://en.wikipedia.org/wiki/Photogrammetry) and it’s surprisingly easy to get started.

Photogrammetry software allows us to take photos or video of environments or objects, and through a semi-automated process, generate 3D models. There are several free photogrammetry applications out there. This particular workflow uses [AliceVision Meshroom](https://alicevision.org/) and is accessible to anyone with a CUDA-capable Nvidia GPU.

With a Pixel 3A, I shot a quick video walking into my kitchen and then around my kitchen table on which I piled with whatver I could find to make things more interesting. The video was a 4K 30 fps MP4 and was recorded vertically with the idea being that orienting the video perpendicular to the plane of motion would capture as much information about the environment as possible by reducing the amount of overlap of each frame as I moved horizontally through the space. I ended up with about 50 seconds of footage (1500 frames) weighing in at 330MB.

From that video I extracted roughly every 10th frame leaving me with just over 160 images, each 2160x3840. The images were then imported into Meshroom and processed with the default pipeline. This took a little over half an hour on an Intel i7-8700K with an Nvidia RTX 2080 and 32GB of memory.

In this video you can see the Meshroom interface - the source images on the left, the resulting point cloud and position of the camera in each source image on the right, and the pipeline below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/UqaD8ys4chU?si=XwUhpq13e71Ff7R2&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

From that 300MB video I ended up with a 1.3 million triangle model weighing in at just over 100MB with the texture (also generated by Meshroom). The final result is unrefined, a little rough, but not bad for an hour of R&D and thirty minutes of processing. With some experimentation it should be possible to get smoother surfaces and fewer gaps in the mesh.

<iframe width="560" height="315" src="https://www.youtube.com/embed/hrMkeMRvcsU?si=U0zcQqkXGWG0cYwQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

I also did a quick walkaround of my dog while he was sleeping on the bed. This model used 25 photos instead of a video and took about four minutes to process.

<iframe width="560" height="315" src="https://www.youtube.com/embed/cjqoGGEJUlc?si=2vNvaHMFVTfLJW8Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>